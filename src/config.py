"""
ì‡¼íŠ¹í—ˆ(Short-Cut) - Configuration Module (Antigravity Edition)
================================================================
Lightweight configuration for OpenAI API + Pinecone Serverless architecture.

Author: Team ë€¨ğŸ’•
License: MIT
"""

from __future__ import annotations

import os
from dataclasses import dataclass, field
from typing import List, Optional
from pathlib import Path

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()


# =============================================================================
# Project Paths
# =============================================================================

PROJECT_ROOT = Path(__file__).parent
DATA_DIR = PROJECT_ROOT / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
TRIPLETS_DIR = DATA_DIR / "triplets"
EMBEDDINGS_DIR = DATA_DIR / "embeddings"
INDEX_DIR = DATA_DIR / "index"

# Create directories if they don't exist
for dir_path in [DATA_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, TRIPLETS_DIR, EMBEDDINGS_DIR, INDEX_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)


# =============================================================================
# BigQuery Configuration
# =============================================================================

@dataclass
class BigQueryConfig:
    """BigQuery connection and query configuration."""
    
    # GCP Project ID (set via environment variable or override here)
    project_id: str = os.environ.get("GCP_PROJECT_ID", "your-gcp-project-id")
    
    # Dataset and table
    dataset: str = "patents-public-data.patents"
    publications_table: str = "publications"  # Full table (~1.4TB scan cost)
    
    # Date range for filtering (cost optimization)
    min_filing_date: str = "2018-01-01"  # Focus on recent patents
    max_filing_date: str = "2024-12-31"
    
    # Batch processing
    batch_size: int = 1000
    max_results: Optional[int] = None  # None = no limit
    
    # Cost optimization
    dry_run: bool = True  # Set to True to check query cost first
    use_query_cache: bool = True
    
    @property
    def full_table_name(self) -> str:
        return f"`{self.dataset}.{self.publications_table}`"


# =============================================================================
# Domain Keywords Configuration
# =============================================================================

@dataclass
class DomainConfig:
    """Technical domain keywords and IPC/CPC codes for filtering."""
    
    # Primary domain name
    domain_name: str = "AI_NLP_Search"
    
    # Search keywords (OR logic) - Broader for more results
    keywords: List[str] = field(default_factory=lambda: [
        # Information retrieval (broad)
        "information retrieval",
        "document retrieval",
        "semantic search",
        "text search",
        
        # NLP/AI core
        "natural language processing",
        "machine learning",
        "neural network",
        "deep learning",
        
        # Embedding/Vector
        "word embedding",
        "text embedding",
        "vector representation",
        
        # Question answering
        "question answering",
        "knowledge base",
    ])
    
    # IPC/CPC Classification Codes
    ipc_codes: List[str] = field(default_factory=lambda: [
        "G06F 16",    # Information retrieval; Database structures
        "G06F 40",    # Natural language processing
        "G06N 3",     # Artificial intelligence - Neural networks
        "G06N 5",     # AI - Knowledge processing
        "G06N 20",    # Machine learning
        "H04L 12",    # Data switching networks (for distributed ML)
    ])
    
    # RAG-specific component keywords for tagging
    rag_component_keywords: List[str] = field(default_factory=lambda: [
        "retriever",
        "generator",
        "reranker",
        "re-ranker",
        "dense passage",
        "sparse retrieval",
        "hybrid retrieval",
        "knowledge base",
        "vector store",
        "embedding index",
        "semantic similarity",
        "context window",
        "chunking",
        "document encoder",
        "query encoder",
    ])


# =============================================================================
# Embedding Model Configuration (OpenAI API)
# =============================================================================

@dataclass
class EmbeddingConfig:
    """OpenAI Embedding configuration."""
    
    # Model - OpenAI text-embedding-3-small
    model_id: str = "text-embedding-3-small"
    embedding_dim: int = 1536  # OpenAI dimension
    max_context_length: int = 8191  # text-embedding-3-small limit
    
    # API settings
    api_key: str = os.environ.get("OPENAI_API_KEY", "")
    
    # Batch processing (OpenAI has 2048 texts per batch limit)
    batch_size: int = 100
    
    # Rate limiting
    requests_per_minute: int = 3000  # OpenAI tier limit
    tokens_per_minute: int = 1_000_000
    
    # Weighting for hybrid indexing
    title_weight: float = 1.5      # Higher weight for titles
    claim_weight: float = 2.0      # Highest weight for claims  
    abstract_weight: float = 1.2   # Medium weight for abstracts
    description_weight: float = 1.0  # Base weight for descriptions




# =============================================================================
# Pinecone Configuration
# =============================================================================

@dataclass
class PineconeConfig:
    """Pinecone vector database configuration (Serverless)."""
    
    # API Key (from environment variable)
    api_key: str = os.environ.get("PINECONE_API_KEY", "")
    
    # Index Settings
    index_name: str = "patent-guard-hybrid"
    dimension: int = 1536  # Must match embedding model
    metric: str = "dotproduct"  # Required for hybrid search (sparse values)
    
    # Cloud Settings (Serverless)
    cloud: str = "aws"
    region: str = "us-east-1"
    
    # Batch processing
    batch_size: int = 100  # Recommended batch size for upsert
    
    # Namespace
    namespace: str = "default"
    
    # Metadata path
    metadata_path: Optional[Path] = None


# =============================================================================
# PAI-NET Triplet Configuration
# =============================================================================

@dataclass
class PAINETConfig:
    """PAI-NET triplet generation configuration."""
    
    # Triplet generation
    min_citations_for_anchor: int = 3  # Minimum citations to be an anchor
    negatives_per_positive: int = 5    # Number of negatives per positive pair
    
    # Negative sampling strategy
    hard_negative_ratio: float = 0.3   # 30% hard negatives (same IPC, no citation)
    random_negative_ratio: float = 0.7 # 70% random negatives
    
    # Output format
    output_format: str = "jsonl"  # jsonl or parquet


# =============================================================================
# Self-RAG Configuration
# =============================================================================

@dataclass
class SelfRAGConfig:
    """Self-RAG analysis configuration using OpenAI."""
    
    # OpenAI API for analysis
    openai_model: str = "gpt-4o-mini"  # Cost-effective, fast
    openai_api_key: str = os.environ.get("OPENAI_API_KEY", "")
    
    # Critique prompt template
    critique_prompt_template: str = """
ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ íŠ¹í—ˆ ë¶„ìŸ ëŒ€ì‘ ì „ë¬¸ ë³€ë¦¬ì‚¬ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ëª©í‘œëŠ” [ë¶„ì„ ëŒ€ìƒ íŠ¹í—ˆ(Anchor)]ê°€ [ì„ í–‰ ê¸°ìˆ (Prior Art)]ì— ì˜í•´ ì‹ ê·œì„±ì´ë‚˜ ì§„ë³´ì„±ì´ ë¶€ì •ë  ìˆ˜ ìˆëŠ”ì§€, í˜¹ì€ ì¹¨í•´ ë¦¬ìŠ¤í¬ê°€ ìˆëŠ”ì§€ë¥¼ 'ë§¤ìš° ë¹„íŒì ì´ê³  ë³´ìˆ˜ì ì¸' ê´€ì ì—ì„œ ì •ë°€ ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

## ë¶„ì„ ì›ì¹™
1. **ì—„ê²©í•œ êµ¬ì„±ìš”ì†Œ ëŒ€ë¹„ (All Elements Rule)**: ì²­êµ¬í•­ì˜ ê° êµ¬ì„±ìš”ì†Œë¥¼ 1:1ë¡œ ëŒ€ë¹„í•˜ì—¬, ë¬¸ì–¸ì  ì¼ì¹˜ ì—¬ë¶€ë¥¼ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ì‹­ì‹œì˜¤. A+B+C êµ¬ì¡°ì—ì„œ Cê°€ ë‹¤ë¥´ë‹¤ë©´ ë¹„ì¹¨í•´ì…ë‹ˆë‹¤.
2. **ì‚¬ì‹¤ ê¸°ë°˜ ë¶„ì„ (Faithfulness)**: ì œê³µëœ í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ì—¬ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
3. **ë¦¬ìŠ¤í¬ ì¤‘ì‹¬ í‰ê°€**: ì„ í–‰ ê¸°ìˆ ì— ìœ ì‚¬í•œ êµ¬ì„±ì´ ì¼ë¶€ë¼ë„ ìˆë‹¤ë©´ ë¦¬ìŠ¤í¬ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•˜ì‹­ì‹œì˜¤. "ëŒ€ì²´ë¡œ ë¹„ìŠ·í•˜ë‹¤"ëŠ” ì‹ì˜ ëª¨í˜¸í•œ í‘œí˜„ì€ ì§€ì–‘í•˜ì‹­ì‹œì˜¤.

## ì…ë ¥ ë°ì´í„°
[ë¶„ì„ ëŒ€ìƒ íŠ¹í—ˆ (Anchor)]
- íŠ¹í—ˆë²ˆí˜¸: {anchor_publication_number}
- í•µì‹¬ ì²­êµ¬í•­: {anchor_claim}

[ì„ í–‰ ê¸°ìˆ  (Prior Art)]
- íŠ¹í—ˆë²ˆí˜¸: {cited_publication_number}
- ê³µê°œ ì²­êµ¬í•­: {cited_claim}

## ë¶„ì„ ìˆ˜í–‰ ìš”ì²­
ë‹¤ìŒ JSON êµ¬ì¡°ì— ë§ì¶° ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì‹­ì‹œì˜¤. (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì‚¬ìš© ê°€ëŠ¥)

[ìœ ì‚¬ë„ í‰ê°€]
- ê¸°ìˆ ì  ìœ ì‚¬ì„± ì ìˆ˜ (0-100ì ). 80ì  ì´ìƒì´ë©´ ê°•ë ¥í•œ ê±°ì ˆì´ìœ /ì¹¨í•´ìœ„í—˜ ì¡´ì¬.
- í•µì‹¬ ê³µí†µ ê¸°ìˆ  ìš”ì†Œ ë‚˜ì—´. (ë¶ˆí•„ìš”í•œ ë°°ê²½ ì„¤ëª… ì œì™¸)

[ì¹¨í•´ ë¦¬ìŠ¤í¬]
- ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: High (ë¬¸ì–¸ ì¹¨í•´ ìœ ë ¥), Medium (ê· ë“± ì¹¨í•´ ê°€ëŠ¥ì„± ë˜ëŠ” ì„¤ê³„ ë³€ê²½ í•„ìš”), Low (êµ¬ì¡°ì  ì°¨ì´ ëª…í™•)
- ìœ„í—˜ ìš”ì†Œ: ì„ í–‰ ê¸°ìˆ ì´ ë¶„ì„ ëŒ€ìƒ íŠ¹í—ˆì˜ ê¶Œë¦¬ ë²”ìœ„ë¥¼ ì ì‹í•˜ëŠ” êµ¬ì²´ì ì¸ ë¶€ë¶„.

[íšŒí”¼ ì „ëµ]
- ë¶„ì„ ëŒ€ìƒ íŠ¹í—ˆê°€ ì„ í–‰ ê¸°ìˆ ì„ íšŒí”¼í•˜ê¸° ìœ„í•´ ìˆ˜ì •í•´ì•¼ í•  êµ¬ì²´ì ì¸ ì„¤ê³„ ë³€ê²½ ì œì•ˆ.
- êµ¬ì„±ìš”ì†Œì˜ ì‚­ì œ, ì¹˜í™˜, ë³€ê²½ì„ í¬í•¨í•œ ì‹¤ì§ˆì  ì¡°ì–¸.
"""
    
    # Output settings
    max_pairs_per_patent: int = 1  # Reduced to save API costs (was 5)
    include_full_context: bool = True  # Include full patent context


# =============================================================================
# Pipeline Configuration
# =============================================================================

@dataclass
class PipelineConfig:
    """Pipeline execution configuration."""
    
    # Concurrency limits (for i5-1340P: 4P + 8E cores)
    max_workers: int = 8  # Limit to prevent UI freezing
    
    # Pre-computation mode
    precompute_embeddings: bool = True
    save_index_to_disk: bool = True


# =============================================================================
# Logging Configuration
# =============================================================================

@dataclass
class LoggingConfig:
    """Logging configuration."""
    
    log_level: str = "INFO"
    log_format: str = "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    log_file: Optional[str] = str(PROJECT_ROOT / "logs" / "patent_guard.log")
    
    # Create logs directory
    def __post_init__(self):
        if self.log_file:
            Path(self.log_file).parent.mkdir(parents=True, exist_ok=True)


# =============================================================================
# Master Configuration
# =============================================================================

@dataclass
class PatentGuardConfig:
    """Master configuration aggregating all sub-configs."""
    
    bigquery: BigQueryConfig = field(default_factory=BigQueryConfig)
    domain: DomainConfig = field(default_factory=DomainConfig)
    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)

    pinecone: PineconeConfig = field(default_factory=PineconeConfig)
    painet: PAINETConfig = field(default_factory=PAINETConfig)
    self_rag: SelfRAGConfig = field(default_factory=SelfRAGConfig)
    pipeline: PipelineConfig = field(default_factory=PipelineConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)


# =============================================================================
# Default Configuration Instance
# =============================================================================

config = PatentGuardConfig()


# =============================================================================
# Configuration Helpers
# =============================================================================

def update_config_from_env() -> PatentGuardConfig:
    """Update configuration from environment variables."""
    global config
    
    # BigQuery
    if os.environ.get("GCP_PROJECT_ID"):
        config.bigquery.project_id = os.environ["GCP_PROJECT_ID"]
    
    # OpenAI API
    if os.environ.get("OPENAI_API_KEY"):
        config.embedding.api_key = os.environ["OPENAI_API_KEY"]
        config.self_rag.openai_api_key = os.environ["OPENAI_API_KEY"]
    
    
    return config


def print_config_summary() -> None:
    """Print configuration summary."""
    print("\n" + "=" * 70)
    print("âš¡ ì‡¼íŠ¹í—ˆ (Short-Cut) v3.0 - Configuration Summary")
    print("=" * 70)
    print(f"\nğŸ“Š BigQuery:")
    print(f"   Project: {config.bigquery.project_id}")
    print(f"   Date Range: {config.bigquery.min_filing_date} ~ {config.bigquery.max_filing_date}")
    print(f"   Dry Run: {config.bigquery.dry_run}")
    
    print(f"\nğŸ” Domain: {config.domain.domain_name}")
    print(f"   Keywords: {len(config.domain.keywords)} terms")
    
    print(f"\nğŸ§  Embedding (OpenAI API):")
    print(f"   Model: {config.embedding.model_id}")
    print(f"   Dimension: {config.embedding.embedding_dim}")
    print(f"   API Key: {'âœ… Set' if config.embedding.api_key else 'âŒ Not set'}")
    
    print(f"\nğŸŒ² Pinecone (Serverless):")
    print(f"   Index Name: {config.pinecone.index_name}")
    print(f"   Cloud: {config.pinecone.cloud} ({config.pinecone.region})")
    print(f"   API Key: {'âœ… Set' if config.pinecone.api_key else 'âŒ Not set'}")
    
    print(f"\nğŸ” Hybrid Search:")
    print(f"   Dense: Pinecone (Cosine)")
    print(f"   Sparse: Local BM25 (rank_bm25)")
    print(f"   Fusion: RRF (k=60)")

    print(f"\nâš¡ Pipeline:")
    print(f"   Max Workers: {config.pipeline.max_workers}")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    update_config_from_env()
    print_config_summary()
