"""
Short-Cut - Self-RAG Patent Agent with Hybrid Search & Streaming
==========================================================================
Advanced RAG pipeline with HyDE, Hybrid Search (RRF), Streaming, and CoT Analysis.

Features:
1. HyDE (Hypothetical Document Embedding) - Generate virtual claims for better retrieval
2. Hybrid Search - Dense (FAISS) + Sparse (BM25) with RRF fusion
3. LLM Streaming Response - Real-time analysis output
4. Critical CoT Analysis - Detailed similarity/infringement/avoidance analysis

Author: Team ë€¨ğŸ’•
License: MIT
"""

from __future__ import annotations

import asyncio
import logging
import os
import re
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, AsyncGenerator

from dotenv import load_dotenv
from pydantic import BaseModel, Field
from openai import AsyncOpenAI
import numpy as np
from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type

load_dotenv()

# Import orjson if available, otherwise fall back to json
try:
    import orjson
    def json_loads(s): return orjson.loads(s)
    def json_dumps(o): return orjson.dumps(o).decode()
except ImportError:
    import json
    json_loads = json.loads
    json_dumps = json.dumps

# =============================================================================
# Logging Setup
# =============================================================================

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =============================================================================
# Configuration (Environment Variables)
# =============================================================================

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

# Models - configurable via environment variables
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
GRADING_MODEL = os.environ.get("GRADING_MODEL", "gpt-4o-mini")  # Cost-effective
ANALYSIS_MODEL = os.environ.get("ANALYSIS_MODEL", "gpt-4o")  # High quality
HYDE_MODEL = os.environ.get("HYDE_MODEL", "gpt-4o-mini")
FALLBACK_MODEL = os.environ.get("FALLBACK_MODEL", "gpt-3.5-turbo")  # Fallback for errors

# Thresholds - configurable via environment variables
GRADING_THRESHOLD = float(os.environ.get("GRADING_THRESHOLD", "0.6"))
MAX_REWRITE_ATTEMPTS = int(os.environ.get("MAX_REWRITE_ATTEMPTS", "1"))
TOP_K_RESULTS = int(os.environ.get("TOP_K_RESULTS", "5"))

# Hybrid search weights
DENSE_WEIGHT = float(os.environ.get("DENSE_WEIGHT", "0.5"))
SPARSE_WEIGHT = float(os.environ.get("SPARSE_WEIGHT", "0.5"))

# Data paths - relative to this file
from pathlib import Path
DATA_DIR = Path(__file__).resolve().parent / "data"
PROCESSED_DIR = DATA_DIR / "processed"
OUTPUT_DIR = DATA_DIR / "outputs"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


# =============================================================================
# Pydantic Models for Structured Outputs
# =============================================================================

class GradingResult(BaseModel):
    """Structured grading result from GPT."""
    patent_id: str = Field(description="Patent publication number")
    score: float = Field(description="Relevance score from 0.0 to 1.0")
    reason: str = Field(description="Brief explanation for the score")


class GradingResponse(BaseModel):
    """Response containing all grading results."""
    results: List[GradingResult] = Field(description="List of grading results")
    average_score: float = Field(description="Average score across all results")


class QueryRewriteResponse(BaseModel):
    """Optimized search query from GPT."""
    optimized_query: str = Field(description="Improved search query")
    keywords: List[str] = Field(description="Key technical terms to search")
    reasoning: str = Field(description="Why this query should work better")


class SimilarityAnalysis(BaseModel):
    """ìœ ì‚¬ë„ í‰ê°€ section."""
    score: int = Field(description="Technical similarity score 0-100")
    common_elements: List[str] = Field(description="Shared technical elements")
    summary: str = Field(description="Overall similarity assessment")
    evidence_patents: List[str] = Field(description="Patent IDs supporting this analysis")


class InfringementAnalysis(BaseModel):
    """ì¹¨í•´ ë¦¬ìŠ¤í¬ section."""
    risk_level: str = Field(description="high, medium, or low")
    risk_factors: List[str] = Field(description="Specific infringement concerns")
    summary: str = Field(description="Overall risk assessment")
    evidence_patents: List[str] = Field(description="Patent IDs supporting this analysis")


class AvoidanceStrategy(BaseModel):
    """íšŒí”¼ ì „ëµ section."""
    strategies: List[str] = Field(description="Design-around approaches")
    alternative_technologies: List[str] = Field(description="Alternative implementations")
    summary: str = Field(description="Recommended avoidance approach")
    evidence_patents: List[str] = Field(description="Patent IDs informing these strategies")


class ComponentComparison(BaseModel):
    """êµ¬ì„±ìš”ì†Œ ëŒ€ë¹„í‘œ - Element-by-element comparison."""
    idea_components: List[str] = Field(description="User idea's key technical components")
    matched_components: List[str] = Field(description="Components found in prior patents")
    unmatched_components: List[str] = Field(description="Novel components not in prior art")
    risk_components: List[str] = Field(description="Components causing infringement risk")


class CriticalAnalysisResponse(BaseModel):
    """Complete critical analysis response."""
    similarity: SimilarityAnalysis
    infringement: InfringementAnalysis
    avoidance: AvoidanceStrategy
    component_comparison: ComponentComparison = Field(description="Element comparison table")
    conclusion: str = Field(description="Final recommendation")


# =============================================================================
# Patent Search Result
# =============================================================================

@dataclass
class PatentSearchResult:
    """A single patent search result."""
    publication_number: str
    title: str
    abstract: str
    claims: str
    ipc_codes: List[str]
    similarity_score: float = 0.0  # Vector similarity
    grading_score: float = 0.0  # LLM grading score
    grading_reason: str = ""
    
    # Hybrid search scores
    dense_score: float = 0.0
    sparse_score: float = 0.0
    rrf_score: float = 0.0
    is_prioritized: bool = False  # Flag for patents explicitly mentioned in query


# =============================================================================
# Patent Agent - Main Class
# =============================================================================

class PatentAgent:
    """
    Self-RAG Patent Analysis Agent (v3.0).
    
    Features:
    - Pinecone Serverless Hybrid Search (Dense + Sparse)
    - OpenAI API for embeddings and LLM
    - Streaming response for real-time analysis
    
    Implements:
    1. HyDE - Hypothetical Document Embedding
    2. Hybrid Search - Dense + Sparse with RRF
    3. Grading & Rewrite Loop
    4. Critical CoT Analysis with Streaming
    """
    
    def __init__(self, db_client=None):
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not set. Check .env file.")
        
        self.client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        
        # Initialize Vector DB client with hybrid search
        if db_client is not None:
            self.db_client = db_client
        else:
            # Use PineconeClient for v3.0 Migration
            from src.vector_db import PineconeClient
            self.db_client = PineconeClient()
            self._try_load_local_cache()
    
    def _try_load_local_cache(self) -> bool:
        """Try to load local metadata cache and BM25 index."""
        loaded = self.db_client.load_local()
        if loaded:
            stats = self.db_client.get_stats()
            logger.info(f"Loaded local cache: {stats.get('bm25_docs', 0)} docs in BM25")
            return True
        else:
            logger.warning("No local cache found. Run pipeline to build BM25 index.")
            return False
    
    def index_loaded(self) -> bool:
        """Check if DB is ready."""
        # For Pinecone, we assume it's always ready if initialized
        return True
    
    # =========================================================================
    # Keyword Extraction for Hybrid Search
    # =========================================================================
    
    async def extract_keywords(self, text: str) -> List[str]:
        """
        Extract keywords from text for BM25 search.
        Uses both rule-based extraction and optional LLM enhancement.
        """
        from src.vector_db import KeywordExtractor
        
        # Rule-based extraction
        keywords = KeywordExtractor.extract(text, max_keywords=15)
        
        return keywords

    def extract_patent_ids(self, text: str) -> List[str]:
        """
        Extract patent IDs (e.g., CN-119821168-A, KR-102842452-B1) from text.
        """
        # Precise pattern for CC-NUMBER-SUFFIX or CC-NUMBER
        pattern = r'\b([A-Z]{2}[-]?\d{4,}(?:[-][A-Z0-9]+)?)\b'
        
        matches = re.findall(pattern, text, re.ASCII)
        # Filter and clean
        cleaned = []
        for m in matches:
            if re.search(r'\d{4,}', m): # Ensure it has enough digits to be a patent ID
                cleaned.append(m.upper())
        
        return list(set(cleaned))
    
    @retry(
        wait=wait_random_exponential(min=1, max=10),
        stop=stop_after_attempt(5),
        retry=retry_if_exception_type(Exception),
    )
    async def _fetch_by_ids_safe(self, ids: List[str]) -> List[Any]:
        """Wrapper for ID fetch with retry AND validation."""
        results = await self.db_client.async_fetch_by_ids(ids)
        
        # Validation: If we requested N IDs, we expect N results (or reasonably close)
        # Note: Pinecone might return fewer if not found, but in our Golden Dataset,
        # we assume all IDs exist. If not found, it's likely a consistency/timeout issue.
        if len(results) < len(ids):
            missing_count = len(ids) - len(results)
            # Create a custom error to trigger retry
            raise ValueError(f"Partial retrieval detected. Requested {len(ids)}, got {len(results)}. Missing {missing_count} items.")
            
        return results

    
    # =========================================================================
    # 1. HyDE - Hypothetical Document Embedding
    # =========================================================================
    
    async def generate_hypothetical_claim(self, user_idea: str) -> str:
        """
        Generate a hypothetical patent claim from user's idea.
        """
        system_prompt = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ íŠ¹í—ˆ ë¶„ìŸ ëŒ€ì‘ ì „ë¬¸ ë³€ë¦¬ì‚¬ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ì¶”ìƒì ì¸ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë²•ì /ê¸°ìˆ ì ìœ¼ë¡œ ê°€ì¥ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ 'ë…ë¦½ ì²­êµ¬í•­(Independent Claim)'ì˜ í˜•íƒœë¡œ ê°€ìƒì˜ íŠ¹í—ˆë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ ê°€ìƒ ì²­êµ¬í•­ì€ ì‹¤ì œ íŠ¹í—ˆ ë°ì´í„°ì…‹ì—ì„œ ìœ ì‚¬í•œ ê¸°ìˆ ì„ ì°¾ì•„ë‚´ê¸° ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤."""

        user_prompt = f"ì•„ì´ë””ì–´: {user_idea}\n\nìœ„ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì „ë¬¸ì ì¸ ê°€ìƒ ì œ1í•­(ë…ë¦½í•­)ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤."

        response = await self.client.chat.completions.create(
            model=HYDE_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=500,
        )
        
        hypothetical_claim = response.choices[0].message.content.strip()
        logger.info(f"Generated hypothetical claim: {hypothetical_claim[:100]}...")
        
        return hypothetical_claim
    
    async def embed_text(self, text: str) -> np.ndarray:
        """Generate embedding using OpenAI text-embedding-3-small."""
        response = await self.client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text,
        )
        return np.array(response.data[0].embedding, dtype=np.float32)
    
    async def generate_multi_queries(self, user_idea: str) -> List[str]:
        """
        Generate multiple search queries for better coverage.
        Returns 3 queries: 
        1. Technical reformulation (synonyms)
        2. Claim-style phrasing
        3. Problem-solution keywords
        """
        system_prompt = """ë‹¹ì‹ ì€ íŠ¹í—ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“íˆê¸° ìœ„í•´ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤:
{
  "queries": [
    "ì¿¼ë¦¬ 1: ì „ë¬¸ ìš©ì–´ ë° ìœ ì˜ì–´ ì¤‘ì‹¬ (Technical Formulation)",
    "ì¿¼ë¦¬ 2: ì²­êµ¬í•­ ìŠ¤íƒ€ì¼ êµ¬ë¬¸ (Claim-style Phrasing)",
    "ì¿¼ë¦¬ 3: í•´ê²°í•˜ë ¤ëŠ” ê³¼ì œì™€ ì†”ë£¨ì…˜ í‚¤ì›Œë“œ (Problem-Solution)"
  ]
}"""
        
        try:
            response = await self.client.chat.completions.create(
                model=HYDE_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_idea}
                ],
                response_format={"type": "json_object"},
                temperature=0.7,
            )
            
            data = json_loads(response.choices[0].message.content)
            queries = data.get("queries", [])
            logger.info(f"Generated {len(queries)} multi-queries")
            return queries[:3]  # Ensure max 3
            
        except Exception as e:
            logger.error(f"Multi-query generation failed: {e}")
            return [user_idea]  # Fallback to original

    async def hyde_search(
        self,
        user_idea: str,
        top_k: int = TOP_K_RESULTS,
        use_hybrid: bool = True,
    ) -> Tuple[str, List[PatentSearchResult]]:
        """
        HyDE-enhanced patent search (Single Query Version).
        """
        # Generate hypothetical claim
        hypothetical_claim = await self.generate_hypothetical_claim(user_idea)
        
        # Check if index is available
        if not self.index_loaded():
            logger.warning("Index not loaded. Returning empty results.")
            return hypothetical_claim, []
            
        results = await self._execute_search(hypothetical_claim, user_idea, top_k, use_hybrid)
        return hypothetical_claim, results

    @retry(
        wait=wait_random_exponential(min=1, max=10),
        stop=stop_after_attempt(3),
        retry=retry_if_exception_type((Exception,)), # Retry on generic exceptions usually network/pinecone related
    )
    async def _execute_search(
        self,
        query_text: str,
        context_text: str,
        top_k: int,
        use_hybrid: bool,
        ipc_filters: List[str] = None
    ) -> List[PatentSearchResult]:
        """Internal helper to execute actual search."""
        # Embed query
        query_embedding = await self.embed_text(query_text)
        
        # Extract keywords
        keywords = await self.extract_keywords(context_text + " " + query_text)
        keyword_query = " ".join(keywords)
        
        # Search
        if use_hybrid:
            search_results = await self.db_client.async_hybrid_search(
                query_embedding,
                keyword_query,
                top_k=top_k,
                dense_weight=DENSE_WEIGHT,
                sparse_weight=SPARSE_WEIGHT,
                ipc_filters=ipc_filters,
            )
        else:
            search_results = await self.db_client.async_search(
                query_embedding, 
                top_k=top_k,
                ipc_filters=ipc_filters,
            )
            
        # Convert objects
        results = []
        for r in search_results:
            results.append(PatentSearchResult(
                publication_number=r.patent_id,
                title=r.metadata.get("title", ""),
                abstract=r.metadata.get("abstract", r.content[:500]),
                claims=r.metadata.get("claims", ""),
                ipc_codes=[r.metadata.get("ipc_code", "")] if r.metadata.get("ipc_code") else [],
                similarity_score=r.score,
                dense_score=getattr(r, 'dense_score', 0.0),
                sparse_score=getattr(r, 'sparse_score', 0.0),
                rrf_score=getattr(r, 'rrf_score', 0.0),
            ))
        return results

    async def search_multi_query(
        self,
        user_idea: str,
        top_k: int = TOP_K_RESULTS,
        use_hybrid: bool = True,
        ipc_filters: List[str] = None,
    ) -> Tuple[List[str], List[PatentSearchResult]]:
        # 1. Detect specific patent IDs in user idea
        target_ids = self.extract_patent_ids(user_idea)
        target_results = []
        if target_ids:
            logger.info(f"Detected target patents in query: {target_ids}")
        if target_ids:
            logger.info(f"Detected target patents in query: {target_ids}")
            raw_target_results = await self._fetch_by_ids_safe(target_ids)

            
            # Convert to PatentSearchResult
            for r in raw_target_results:
                target_results.append(PatentSearchResult(
                    publication_number=r.patent_id,
                    title=r.metadata.get("title", ""),
                    abstract=r.metadata.get("abstract", r.content[:500]),
                    claims=r.metadata.get("claims", ""),
                    ipc_codes=[r.metadata.get("ipc_code", "")] if r.metadata.get("ipc_code") else [],
                    similarity_score=r.score,
                    dense_score=getattr(r, 'dense_score', 0.0),
                    sparse_score=getattr(r, 'sparse_score', 0.0),
                    rrf_score=getattr(r, 'rrf_score', 0.0),
                    is_prioritized=True,  # Mark as prioritized
                ))
            logger.info(f"Found {len(target_results)} requested patents in DB")

        # 2. Generate queries for broader search
        queries = await self.generate_multi_queries(user_idea)
        if not queries:
            queries = [user_idea]
            
        logger.info(f"Executing Multi-Query Search with: {queries}")
        
        # 3. Parallel Execution using asyncio.gather
        tasks = [
            self._execute_search(query, user_idea, top_k, use_hybrid, ipc_filters=ipc_filters)
            for query in queries
        ]
        
        results_list = await asyncio.gather(*tasks)
        
        # 4. Deduplication & Fusion
        seen_ids = set()
        merged_results = []
        
        # Pre-populate with target results so they are definitely included
        for r in target_results:
            if r.publication_number not in seen_ids:
                seen_ids.add(r.publication_number)
                r.is_prioritized = True
                merged_results.append(r)

        # Simple Fusion: Round-Robin or Score-based?
        # Using Score-based here (Flatten and sort by RRF/Sim score)
        all_results = [item for sublist in results_list for item in sublist]
        
        # Sort by score descending before dedup to keep highest scoring instance
        all_results.sort(key=lambda x: x.rrf_score if use_hybrid else x.similarity_score, reverse=True)
        
        for r in all_results:
            if r.publication_number not in seen_ids:
                seen_ids.add(r.publication_number)
                merged_results.append(r)
            else:
                # If it's a target patent seen again, ensure the is_prioritized flag is preserved
                # if it was already marked as such in merged_results
                pass
        
        logger.info(f"Multi-Query: {len(all_results)} total -> {len(merged_results)} unique results")
        return queries, merged_results[:top_k*2]  # Return more candidates for grading
    
    # =========================================================================
    # 2. Grading & Rewrite Loop
    # =========================================================================
    
    async def grade_results(
        self,
        user_idea: str,
        results: List[PatentSearchResult],
    ) -> GradingResponse:
        """Grade each search result for relevance to user's idea."""
        if not results:
            return GradingResponse(results=[], average_score=0.0)
        
        results_text = "\n\n".join([
            f"[íŠ¹í—ˆ {i+1}: {r.publication_number}]\n"
            f"ì œëª©: {r.title}\n"
            f"ì´ˆë¡: {r.abstract[:300]}...\n"
            f"ì²­êµ¬í•­: {r.claims[:300]}..."
            for i, r in enumerate(results)
        ])
        
        system_prompt = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ íŠ¹í—ˆ ë¶„ìŸ ëŒ€ì‘ ì „ë¬¸ ë³€ë¦¬ì‚¬ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ê²€ìƒ‰ëœ íŠ¹í—ˆê°€ ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ì™€ ê¸°ìˆ ì ìœ¼ë¡œ ì‹¤ì§ˆì ì¸ ê´€ë ¨ì´ ìˆëŠ”ì§€ë¥¼ 'ë§¤ìš° ë¹„íŒì ì´ê³  ë³´ìˆ˜ì ì¸' ê´€ì ì—ì„œ í‰ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

í‰ê°€ ì§€ì¹¨:
1. **ê¸°ìˆ ì  ì‹¤í˜„ ê°€ëŠ¥ì„± ë° ë…¼ë¦¬**: ì•„ì´ë””ì–´ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ ì„±ë¦½í•˜ì§€ ì•Šê±°ë‚˜(ì˜ˆ: ì „í˜€ ë‹¤ë¥¸ ì„±ì§ˆì˜ ê¸°ìˆ ì´ ë¬¼ë¦¬ì /ìƒë¬¼í•™ì ìœ¼ë¡œ ê²°í•© ë¶ˆê°€í•œ ê²½ìš°), ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ì§œì§‘ê¸°ì¸ ê²½ìš° ë‚®ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì‹­ì‹œì˜¤.
2. **ê¸°ìˆ  ë¶„ì•¼ ë° ëª©ì **: ì•„ì´ë””ì–´ì˜ 'ì§„ì •í•œ ê¸°ìˆ ì  ê³¼ì œ'ì™€ íŠ¹í—ˆì˜ 'í•´ê²°í•˜ë ¤ëŠ” ê³¼ì œ'ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ ìš°ì„ ìˆœìœ„ë¥¼ ë‘ì‹­ì‹œì˜¤.
3. **í‰ê°€ ê¸°ì¤€ (0.0 ~ 1.0 ì )**:
   - 0.8~1.0: ê¸°ìˆ ì  ìˆ˜ë‹¨ê³¼ ëª©ì ì´ ê±°ì˜ ë™ì¼í•¨ (ì§ì ‘ì  ì¹¨í•´ ë¦¬ìŠ¤í¬)
   - 0.5~0.7: ê¸°ìˆ  ë¶„ì•¼ëŠ” ê°™ìœ¼ë‚˜ ì„¸ë¶€ êµ¬í˜„ ë°©ì‹ì´ ë‹¤ë¦„ (ê°œëŸ‰ ë˜ëŠ” íšŒí”¼ ê°€ëŠ¥ì„±)
   - 0.1~0.4: í‚¤ì›Œë“œë§Œ ê²¹ì¹˜ê±°ë‚˜ ê¸°ìˆ ì  ë§¥ë½ì´ ìƒì´í•¨ (ë‹¨ìˆœ ì°¸ê³  ìˆ˜ì¤€)
   - 0.0: ê¸°ìˆ ì ìœ¼ë¡œ ë¬´ê´€í•¨

í‰ê°€ ì‹œ 'ì˜¤ì´ë§› ì†Œê³ ê¸°'ì™€ ê°™ì´ í‚¤ì›Œë“œ(ìœ¡ì¢…, ì†Œê³ ê¸°, ì˜¤ì´)ëŠ” ì¡´ì¬í•˜ë‚˜ ê¸°ìˆ ì  ì‹¤ì²´ê°€ ë¶ˆë¶„ëª…í•˜ê±°ë‚˜ ë…¼ë¦¬ì  ë¹„ì•½ì´ ìˆëŠ” ê²½ìš°, ìœ ì‚¬ë„ê°€ ë†’ê²Œ ì¸¡ì •ë˜ì§€ ì•Šë„ë¡ ì—„ê²©í•˜ê²Œ ì‹¬ì‚¬í•˜ì‹­ì‹œì˜¤.
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤."""

        user_prompt = f"""[ì‚¬ìš©ì ì•„ì´ë””ì–´]
{user_idea}

[ê²€ìƒ‰ëœ íŠ¹í—ˆ ëª©ë¡]
{results_text}

ê° íŠ¹í—ˆì— ëŒ€í•´ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€í•˜ì‹­ì‹œì˜¤:
{{
  "results": [
    {{"patent_id": "íŠ¹í—ˆë²ˆí˜¸", "score": 0.0-1.0, "reason": "í‰ê°€ ì´ìœ "}}
  ],
  "average_score": ì „ì²´í‰ê· ì ìˆ˜
}}"""

        response = await self.client.chat.completions.create(
            model=GRADING_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.1,
        )
        
        try:
            grading_data = json_loads(response.choices[0].message.content)
            grading_response = GradingResponse(**grading_data)
            
            for grade in grading_response.results:
                for result in results:
                    if result.publication_number == grade.patent_id:
                        # Priority Boost: If explicitly requested, force score to 1.0
                        if result.is_prioritized:
                            result.grading_score = 1.0
                            result.grading_reason = f"[PRIORITIZED] {grade.reason}"
                        else:
                            result.grading_score = grade.score
                            result.grading_reason = grade.reason
            
            # Failsafe: Ensure prioritized results are ALWAYS boosted, even if LLM omitted them
            for result in results:
                if result.is_prioritized:
                    result.grading_score = 1.0
                    if not result.grading_reason:
                        result.grading_reason = "[PRIORITIZED] Explicitly requested by user"
                    elif "[PRIORITIZED]" not in result.grading_reason:
                         result.grading_reason = f"[PRIORITIZED] {result.grading_reason}"
            
            return grading_response
            
        except Exception as e:
            logger.error(f"Failed to parse grading response: {e}")
            # Even on error, return prioritized results
            for result in results:
                if result.is_prioritized:
                    result.grading_score = 1.0
                    result.grading_reason = "[PRIORITIZED] Grading failed but ID matched"
            return GradingResponse(results=[], average_score=0.0)
    
    async def rewrite_query(
        self,
        user_idea: str,
        previous_results: List[PatentSearchResult],
    ) -> QueryRewriteResponse:
        """Optimize search query based on poor results."""
        results_summary = "\n".join([
            f"- {r.publication_number}: score={r.grading_score:.2f}, {r.grading_reason}"
            for r in previous_results
        ])
        
        prompt = f"""ê²€ìƒ‰ ê²°ê³¼ê°€ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”.

[ì›ë˜ ì•„ì´ë””ì–´]
{user_idea}

[ì´ì „ ê²€ìƒ‰ ê²°ê³¼ (ë‚®ì€ ì ìˆ˜)]
{results_summary}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "optimized_query": "ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬",
  "keywords": ["í•µì‹¬", "ê¸°ìˆ ", "í‚¤ì›Œë“œ"],
  "reasoning": "ê°œì„  ì´ìœ "
}}"""

        response = await self.client.chat.completions.create(
            model=GRADING_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.3,
        )
        
        try:
            data = json_loads(response.choices[0].message.content)
            return QueryRewriteResponse(**data)
        except Exception as e:
            logger.error(f"Failed to parse rewrite response: {e}")
            return QueryRewriteResponse(
                optimized_query=user_idea,
                keywords=[],
                reasoning="Failed to optimize"
            )
    
    async def search_with_grading(
        self,
        user_idea: str,
        use_hybrid: bool = True,
    ) -> List[PatentSearchResult]:
        """Complete search pipeline with grading and optional rewrite."""
        # Initial Search (Multi-Query handles ID prioritization)
        queries, results = await self.search_multi_query(user_idea, use_hybrid=use_hybrid)
        
        if not results:
            logger.warning("No search results found")
            return []
        
        # Grade results
        grading = await self.grade_results(user_idea, results)
        logger.info(f"Initial grading - Average score: {grading.average_score:.2f}")
        
        # Check if rewrite is needed
        if grading.average_score < GRADING_THRESHOLD:
            logger.info(f"Score below threshold ({GRADING_THRESHOLD}), attempting query rewrite...")
            
            rewrite = await self.rewrite_query(user_idea, results)
            logger.info(f"Rewritten query: {rewrite.optimized_query}")
            
            _, new_results = await self.search_multi_query(rewrite.optimized_query, use_hybrid=use_hybrid)
            
            new_grading = await self.grade_results(user_idea, new_results)
            logger.info(f"After rewrite - Average score: {new_grading.average_score:.2f}")
            
            if new_grading.average_score > grading.average_score:
                results = new_results
                grading = new_grading
        
        results.sort(key=lambda x: x.grading_score, reverse=True)
        
        return results
    
    # =========================================================================
    # 3. Critical CoT Analysis - Standard (Non-Streaming)
    # =========================================================================
    
    async def critical_analysis(
        self,
        user_idea: str,
        results: List[PatentSearchResult],
    ) -> CriticalAnalysisResponse:
        """
        Perform critical Chain-of-Thought analysis (non-streaming).
        """
        if not results:
            return self._empty_analysis()
        
        # Filter out low-quality results to prevent hallucinations
        # We only analyze patents that have a minimum baseline relevance.
        relevant_results = [r for r in results if r.grading_score >= 0.3][:5]
        
        if not relevant_results:
            # If no results are good enough, we still want to inform the user
            # rather than failing silently or hallucinating.
            patents_text = "ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ë¶„ì„í•  ê°€ì¹˜ê°€ ìˆëŠ”(ì ìˆ˜ 0.3 ì´ìƒ) ê´€ë ¨ íŠ¹í—ˆê°€ ì—†ìŠµë‹ˆë‹¤."
        else:
            patents_text = "\n\n".join([
                f"=== íŠ¹í—ˆ {r.publication_number} ===\n"
                f"ì œëª©: {r.title}\n"
                f"IPC: {', '.join(r.ipc_codes[:3])}\n"
                f"ì´ˆë¡: {r.abstract}\n"
                f"ì²­êµ¬í•­: {r.claims}\n"
                f"ê´€ë ¨ì„± ì ìˆ˜: {r.grading_score:.2f} ({r.grading_reason})"
                for r in relevant_results
            ])

        
        system_prompt, user_prompt = self._build_analysis_prompts(user_idea, patents_text)
        
        try:
            response = await self.client.chat.completions.create(
                model=ANALYSIS_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_tokens=2500,
            )
            
            data = json_loads(response.choices[0].message.content)
            return CriticalAnalysisResponse(**data)
            
        except Exception as e:
            logger.error(f"Analysis failed with {ANALYSIS_MODEL}: {e}")
            logger.warning(f"Falling back to {FALLBACK_MODEL}...")
            
            try:
                # Fallback implementation
                response = await self.client.chat.completions.create(
                    model=FALLBACK_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.2,
                    max_tokens=2500,
                )
                
                data = json_loads(response.choices[0].message.content)
                return CriticalAnalysisResponse(**data)
            except Exception as fallback_error:
                logger.error(f"Fallback analysis failed: {fallback_error}")
                return self._empty_analysis()
    
    # =========================================================================
    # 4. Critical CoT Analysis - Streaming
    # =========================================================================
    
    async def critical_analysis_stream(
        self,
        user_idea: str,
        results: List[PatentSearchResult],
    ) -> AsyncGenerator[str, None]:
        """
        Perform critical Chain-of-Thought analysis with streaming.
        
        Yields:
            Tokens as they are generated by the LLM
        """
        if not results:
            yield "ë¶„ì„í•  íŠ¹í—ˆê°€ ì—†ìŠµë‹ˆë‹¤."
            return
        
        # Filter out low-quality results to prevent hallucinations
        relevant_results = [r for r in results if r.grading_score >= 0.3][:5]
        
        if not relevant_results:
            patents_text = "ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ë¶„ì„í•  ê°€ì¹˜ê°€ ìˆëŠ”(ì ìˆ˜ 0.3 ì´ìƒ) ê´€ë ¨ íŠ¹í—ˆê°€ ì—†ìŠµë‹ˆë‹¤."
        else:
            patents_text = "\n\n".join([
                f"=== íŠ¹í—ˆ {r.publication_number} ===\n"
                f"ì œëª©: {r.title}\n"
                f"IPC: {', '.join(r.ipc_codes[:3])}\n"
                f"ì´ˆë¡: {r.abstract[:500]}\n"
                f"ì²­êµ¬í•­: {r.claims[:500]}\n"
                f"ê´€ë ¨ì„± ì ìˆ˜: {r.grading_score:.2f}"
                for r in relevant_results
            ])

        
        system_prompt = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ íŠ¹í—ˆ ë¶„ìŸ ëŒ€ì‘ ì „ë¬¸ ë³€ë¦¬ì‚¬ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì œê³µëœ ì„ í–‰ íŠ¹í—ˆ(Context)ì™€ ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ë¥¼ 'ë§¤ìš° ë¹„íŒì ì´ê³  ë³´ìˆ˜ì ì¸' ê´€ì ì—ì„œ ëŒ€ë¹„í•˜ì—¬ ì¹¨í•´ ë¦¬ìŠ¤í¬ì™€ ê¸°ìˆ ì  ìœ ì‚¬ë„ë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë¶„ì„ ì›ì¹™ (CRITICAL):
1. **ì‚¬ì‹¤ì—ë§Œ ê¸°ë°˜ (Strict Faithfulness)**: 
   - ì˜¤ì§ ì•„ë˜ [Context]ì— ì œê³µëœ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
   - **ì ˆëŒ€ Contextì— ì—†ëŠ” ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤ (NEVER FABRICATE).**
   - [íŠ¹í—ˆë²ˆí˜¸]ë¥¼ ë³´ê³  ë‹¹ì‹ ì˜ í•™ìŠµ ë°ì´í„°ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆì§€ì…ë‹ˆë‹¤.
   - Contextì— ëª…ì‹œë˜ì§€ ì•Šì€ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ ì¶”ì¸¡í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

2. **ëª…ì‹œì  ì¸ìš© ì˜ë¬´ (Explicit Citation)**:
   - ëª¨ë“  ë¶„ì„ ì£¼ì¥ì—ëŠ” ë°˜ë“œì‹œ [íŠ¹í—ˆë²ˆí˜¸]ë¥¼ ë³‘ê¸°í•˜ì‹­ì‹œì˜¤.
   - ì¸ìš©í•  íŠ¹í—ˆê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ì£¼ì¥ì„ í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

3. **ë¶ˆí™•ì‹¤ì„± ì¸ì • (Acknowledge Uncertainty)**:
   - Contextì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì •ë³´ ë¶€ì¡±" ë˜ëŠ” "N/A"ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤.

4. **ì—„ê²©í•œ êµ¬ì„±ìš”ì†Œ ëŒ€ë¹„ (All Elements Rule)**: 
   - ì²­êµ¬í•­ì˜ ê° êµ¬ì„±ìš”ì†Œë¥¼ 1:1ë¡œ ëŒ€ë¹„í•˜ì—¬, ë¬¸ì–¸ì  ì¼ì¹˜ ì—¬ë¶€ë¥¼ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ì‹­ì‹œì˜¤.




**ì¤‘ìš”**: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

ë¶„ì„ ì ˆì°¨:
1. **ì²­êµ¬í•­ íŠ¹ì •**: ê° íŠ¹í—ˆì—ì„œ ê°€ì¥ ì¹¨í•´ ìœ„í—˜ì´ ë†’ì€ 'ëŒ€í‘œ ì²­êµ¬í•­'ì„ í•˜ë‚˜ì”© íŠ¹ì •í•˜ì‹­ì‹œì˜¤.
2. **êµ¬ì„±ìš”ì†Œ ëŒ€ë¹„ (All Elements Rule)**: 
   - ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ê°€ ì„ í–‰ íŠ¹í—ˆ ì²­êµ¬í•­ì˜ ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ í¬í•¨í•˜ëŠ”ì§€ ê²€í† í•˜ì‹­ì‹œì˜¤.
   - í•˜ë‚˜ë¼ë„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©´ ë¹„ì¹¨í•´(íšŒí”¼ ê°€ëŠ¥)ë¡œ íŒë‹¨í•˜ì‹­ì‹œì˜¤.
3. **ì¹¨í•´ ë¦¬ìŠ¤í¬ íŒì •**: 
   - High: ì•„ì´ë””ì–´ì— ì²­êµ¬í•­ì˜ ëª¨ë“  êµ¬ì„±ìš”ì†Œê°€ í¬í•¨ë¨ (ë¬¸ì–¸ ì¹¨í•´ ìœ„í—˜)
   - Medium: ì¼ë¶€ êµ¬ì„±ìš”ì†Œê°€ ê· ë“±ë¬¼ë¡œ ì¹˜í™˜ ê°€ëŠ¥í•¨ (ê· ë“± ì¹¨í•´ ìœ„í—˜)
   - Low: ì²­êµ¬í•­ì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œê°€ ì•„ì´ë””ì–´ì— ì—†ìŒ (ììœ  ì‹¤ì‹œ ê°€ëŠ¥)

ì¶œë ¥ í˜•ì‹ (ë§ˆí¬ë‹¤ìš´):
## 1. ìœ ì‚¬ë„ í‰ê°€
- **í•µì‹¬ ê¸°ìˆ **: (ì•„ì´ë””ì–´ ì •ì˜)
- **ì¢…í•© ì ìˆ˜**: (0-100ì )
- (íŠ¹í—ˆë³„ ê°„ë‹¨ ì½”ë©˜íŠ¸)

## 2. ì²­êµ¬í•­ ê¸°ë°˜ ì¹¨í•´ ë¦¬ìŠ¤í¬
â€» ê° íŠ¹í—ˆë³„ë¡œ ê°€ì¥ ìœ„í—˜í•œ ì²­êµ¬í•­ì„ ë¶„ì„í•©ë‹ˆë‹¤.

### [íŠ¹í—ˆë²ˆí˜¸] ì œëª©
- **ìœ„í—˜ ì²­êµ¬í•­**: (ì˜ˆ: ì œ1í•­)
- **êµ¬ì„±ìš”ì†Œ ëŒ€ë¹„**:
  - [ì•„ì´ë””ì–´ êµ¬ì„±] vs [ì²­êµ¬í•­ êµ¬ì„±] â†’ **ì¼ì¹˜/ë¶ˆì¼ì¹˜**
  - (ë¶ˆì¼ì¹˜ ì‹œ ì´ìœ  ì„¤ëª…)
- **ë¦¬ìŠ¤í¬**: ğŸ”´ High / ğŸŸ¡ Medium / ğŸŸ¢ Low

(ë‹¤ë¥¸ íŠ¹í—ˆë“¤ë„ ë™ì¼í•˜ê²Œ ë°˜ë³µ...)

## 3. íšŒí”¼ ì „ëµ
(íšŒí”¼ ì„¤ê³„ ì œì•ˆ)

## 4. ê²°ë¡ 
(ìµœì¢… ê¶Œê³ )"""

        user_prompt = f"""[ë¶„ì„ ëŒ€ìƒ: ì‚¬ìš©ì ì•„ì´ë””ì–´]
{user_idea}

[ì°¸ì¡° íŠ¹í—ˆ ëª©ë¡ (ì„ í–‰ ê¸°ìˆ )]
{patents_text}

ìœ„ ì„ í–‰ íŠ¹í—ˆë“¤ì˜ **ì²­êµ¬í•­(Claims)**ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì•„ì´ë””ì–´ì™€ ì •ë°€ ëŒ€ë¹„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤."""

        response = await self.client.chat.completions.create(
            model=ANALYSIS_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            stream=True,
            temperature=0.2,
            max_tokens=2500,
        )
        
        async for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    
    def _build_analysis_prompts(self, user_idea: str, patents_text: str) -> Tuple[str, str]:
        """Build system and user prompts for analysis."""
        system_prompt = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ íŠ¹í—ˆ ë¶„ìŸ ëŒ€ì‘ ì „ë¬¸ ë³€ë¦¬ì‚¬ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì œê³µëœ ì„ í–‰ íŠ¹í—ˆ(Context)ì™€ ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ë¥¼ ëŒ€ë¹„í•˜ì—¬, ì‹ ê·œì„±ì´ë‚˜ ì§„ë³´ì„±ì´ ë¶€ì •ë  ìˆ˜ ìˆëŠ”ì§€ í˜¹ì€ ì¹¨í•´ ë¦¬ìŠ¤í¬ê°€ ìˆëŠ”ì§€ë¥¼ 'ë§¤ìš° ë¹„íŒì ì´ê³  ë³´ìˆ˜ì ì¸' ê´€ì ì—ì„œ ì •ë°€ ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë¶„ì„ ì›ì¹™ (CRITICAL):
1. **ì‚¬ì‹¤ì—ë§Œ ê¸°ë°˜ (Strict Faithfulness)**: 
   - ì˜¤ì§ ì•„ë˜ [Context]ì— ì œê³µëœ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
   - **ì ˆëŒ€ Contextì— ì—†ëŠ” ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤ (NEVER FABRICATE).**
   - íŠ¹í—ˆ ë²ˆí˜¸ë¥¼ ë³´ê³  ë‹¹ì‹ ì˜ í•™ìŠµ ë°ì´í„°ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆì§€ì…ë‹ˆë‹¤.
   - Contextì— ëª…ì‹œë˜ì§€ ì•Šì€ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ ì¶”ì¸¡í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

2. **ëª…ì‹œì  ì¸ìš© ì˜ë¬´ (Explicit Citation)**:
   - ëª¨ë“  ë¶„ì„ ì£¼ì¥ì—ëŠ” ë°˜ë“œì‹œ [íŠ¹í—ˆë²ˆí˜¸]ë¥¼ ë³‘ê¸°í•˜ì‹­ì‹œì˜¤.
   - ì˜ˆ: "ë²¡í„° ê²€ìƒ‰ ê¸°ìˆ ì´ ìœ ì‚¬í•©ë‹ˆë‹¤ [CN-12345]"
   - ì¸ìš©í•  íŠ¹í—ˆê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ì£¼ì¥ì„ í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

3. **ë¶ˆí™•ì‹¤ì„± ì¸ì • (Acknowledge Uncertainty)**:
   - Contextì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "Contextì— ëª…ì‹œë˜ì§€ ì•ŠìŒ" ë˜ëŠ” "ì •ë³´ ë¶€ì¡±"ìœ¼ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤.
   - ì¶”ì¸¡í•˜ê¸°ë³´ë‹¤ N/Aë¡œ í‘œê¸°í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•œ ë¶„ì„ì…ë‹ˆë‹¤.

4. **ì—„ê²©í•œ êµ¬ì„±ìš”ì†Œ ëŒ€ë¹„ (All Elements Rule)**: 
   - ì²­êµ¬í•­ì˜ ê° êµ¬ì„±ìš”ì†Œë¥¼ 1:1ë¡œ ëŒ€ë¹„í•˜ì—¬, ë¬¸ì–¸ì  ì¼ì¹˜ ì—¬ë¶€ë¥¼ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ì‹­ì‹œì˜¤.
"""


        user_prompt = f"""[ë¶„ì„ ëŒ€ìƒ: ì‚¬ìš©ì ì•„ì´ë””ì–´]
{user_idea}

[ì°¸ì¡° íŠ¹í—ˆ ëª©ë¡ (ì„ í–‰ ê¸°ìˆ )]
{patents_text}

ìœ„ ì„ í–‰ íŠ¹í—ˆë“¤ê³¼ ì‚¬ìš©ì ì•„ì´ë””ì–´ë¥¼ ëŒ€ë¹„ ë¶„ì„í•˜ì—¬ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤:
{{
  "similarity": {{
    "score": 0-100,
    "common_elements": ["ê³µí†µ êµ¬ì„±ìš”ì†Œ"],
    "summary": "ë¶„ì„ ê²°ê³¼",
    "evidence_patents": ["íŠ¹í—ˆë²ˆí˜¸"]
  }},
  "infringement": {{
    "risk_level": "high/medium/low",
    "risk_factors": ["ìœ„í—˜ ìš”ì†Œ"],
    "summary": "ë¦¬ìŠ¤í¬ í‰ê°€",
    "evidence_patents": ["íŠ¹í—ˆë²ˆí˜¸"]
  }},
  "avoidance": {{
    "strategies": ["íšŒí”¼ ì „ëµ"],
    "alternative_technologies": ["ëŒ€ì•ˆ ê¸°ìˆ "],
    "summary": "íšŒí”¼ ê¶Œê³ ",
    "evidence_patents": ["íŠ¹í—ˆë²ˆí˜¸"]
  }},
  "component_comparison": {{
    "idea_components": ["ì•„ì´ë””ì–´ êµ¬ì„±ìš”ì†Œ"],
    "matched_components": ["ì¼ì¹˜ êµ¬ì„±ìš”ì†Œ"],
    "unmatched_components": ["ì‹ ê·œ êµ¬ì„±ìš”ì†Œ"],
    "risk_components": ["ìœ„í—˜ êµ¬ì„±ìš”ì†Œ"]
  }},
  "conclusion": "ìµœì¢… ê¶Œê³ "
}}"""
        
        return system_prompt, user_prompt
    
    def _empty_analysis(self) -> CriticalAnalysisResponse:
        """Return empty analysis when no results."""
        return CriticalAnalysisResponse(
            similarity=SimilarityAnalysis(
                score=0,
                common_elements=[],
                summary="ë¶„ì„í•  íŠ¹í—ˆê°€ ì—†ìŠµë‹ˆë‹¤.",
                evidence_patents=[]
            ),
            infringement=InfringementAnalysis(
                risk_level="unknown",
                risk_factors=[],
                summary="ë¶„ì„í•  íŠ¹í—ˆê°€ ì—†ìŠµë‹ˆë‹¤.",
                evidence_patents=[]
            ),
            avoidance=AvoidanceStrategy(
                strategies=[],
                alternative_technologies=[],
                summary="ë¶„ì„í•  íŠ¹í—ˆê°€ ì—†ìŠµë‹ˆë‹¤.",
                evidence_patents=[]
            ),
            component_comparison=ComponentComparison(
                idea_components=[],
                matched_components=[],
                unmatched_components=[],
                risk_components=[]
            ),
            conclusion="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
    
    # =========================================================================
    # Main Pipeline
    # =========================================================================
    
    async def analyze(
        self,
        user_idea: str,
        use_hybrid: bool = True,
        stream: bool = False,
    ) -> Dict[str, Any]:
        """
        Complete Self-RAG pipeline.
        
        Args:
            user_idea: User's patent idea
            use_hybrid: Use hybrid search (dense + sparse)
            stream: Stream analysis output (not applicable for dict output)
        """
        print("\n" + "=" * 70)
        print("âš¡ ì‡¼íŠ¹í—ˆ (Short-Cut) v3.0 - Self-RAG Analysis (Hybrid + Streaming)")
        print("=" * 70)
        
        print(f"\nğŸ“ User Idea: {user_idea[:100]}...")
        
        print("\nğŸ” Step 1-2: HyDE + Hybrid Search & Grading...")
        results = await self.search_with_grading(user_idea, use_hybrid=use_hybrid)
        
        if not results:
            return {"error": "No relevant patents found"}
        
        print(f"   Found {len(results)} relevant patents")
        for r in results[:3]:
            print(f"   - {r.publication_number}: {r.grading_score:.2f} (RRF: {r.rrf_score:.4f})")
        
        print("\nğŸ§  Step 3: Critical CoT Analysis...")
        analysis = await self.critical_analysis(user_idea, results)
        
        output = {
            "user_idea": user_idea,
            "search_results": [
                {
                    "patent_id": r.publication_number,
                    "title": r.title,
                    "abstract": r.abstract,  # Added for DeepEval Faithfulness
                    "claims": r.claims,      # Added for DeepEval Faithfulness
                    "grading_score": r.grading_score,
                    "grading_reason": r.grading_reason,
                    "dense_score": r.dense_score,
                    "sparse_score": r.sparse_score,
                    "rrf_score": r.rrf_score,
                }
                for r in results
            ],
            "analysis": {
                "similarity": {
                    "score": analysis.similarity.score,
                    "common_elements": analysis.similarity.common_elements,
                    "summary": analysis.similarity.summary,
                    "evidence": analysis.similarity.evidence_patents,
                },
                "infringement": {
                    "risk_level": analysis.infringement.risk_level,
                    "risk_factors": analysis.infringement.risk_factors,
                    "summary": analysis.infringement.summary,
                    "evidence": analysis.infringement.evidence_patents,
                },
                "avoidance": {
                    "strategies": analysis.avoidance.strategies,
                    "alternatives": analysis.avoidance.alternative_technologies,
                    "summary": analysis.avoidance.summary,
                    "evidence": analysis.avoidance.evidence_patents,
                },
                "conclusion": analysis.conclusion,
            },
            "timestamp": datetime.now().isoformat(),
            "search_type": "hybrid" if use_hybrid else "dense",
        }
        
        print("\n" + "=" * 70)
        print("ğŸ“Š Analysis Complete!")
        print("=" * 70)
        print(f"\n[ìœ ì‚¬ë„ í‰ê°€] Score: {analysis.similarity.score}/100")
        print(f"\n[ì¹¨í•´ ë¦¬ìŠ¤í¬] Level: {analysis.infringement.risk_level.upper()}")
        print(f"\nğŸ“Œ Conclusion: {analysis.conclusion[:150]}...")
        
        return output


# =============================================================================
# CLI Entry Point
# =============================================================================

async def main():
    """Interactive CLI for patent analysis."""
    print("\n" + "=" * 70)
    print("âš¡ ì‡¼íŠ¹í—ˆ (Short-Cut) v3.0 - Self-RAG Patent Agent")
    print("    Hybrid Search + Streaming Edition")
    print("=" * 70)
    print("\níŠ¹í—ˆ ë¶„ì„ì„ ìœ„í•œ ì•„ì´ë””ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ì„ ì…ë ¥í•˜ì„¸ìš”.\n")
    
    agent = PatentAgent()
    
    if not agent.index_loaded():
        print("âš ï¸  Index not found. Please run the pipeline first:")
        print("   python pipeline.py --stage 5\n")
    
    while True:
        try:
            user_input = input("\nğŸ’¡ Your idea: ").strip()
            
            if user_input.lower() in ['exit', 'quit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if not user_input:
                print("âŒ Please enter an idea.")
                continue
            
            result = await agent.analyze(user_input, use_hybrid=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = OUTPUT_DIR / f"analysis_{timestamp}.json"
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(json_dumps(result))
            
            print(f"\nğŸ’¾ Result saved to: {output_path}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")


if __name__ == "__main__":
    asyncio.run(main())
